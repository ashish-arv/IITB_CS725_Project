{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities:\n",
    "def load_spectrum(filename):\n",
    "  number_channels = 1016\n",
    "  # print(f'loading data from : \"{filename}\"')\n",
    "  f = open(filename, 'r')\n",
    "  data = [[float(item2) for item2 in item.split('\\t') if len(item2) > 0]\n",
    "    for item in f.read().split('\\n') if len(item) > 0]\n",
    "  data_unique = set([len(item) for item in data])\n",
    "  if len(data) != number_channels:\n",
    "    print(f'ERROR: Total number of channels is not {number_channels}.')\n",
    "    exit()\n",
    "  elif data_unique == {2} or data_unique == {3}:\n",
    "    energy = np.array([item[0] for item in data])\n",
    "    counts = np.array([item[1] for item in data])\n",
    "    if 'Exp' in filename:\n",
    "      # i.e., if spectrum is NaI generated, normalize w.r.t aquisition time:\n",
    "      aquisition_time = int([item for item in filename.split('_') if 'sec' in item][0].split('sec')[0])\n",
    "      counts = counts/aquisition_time\n",
    "    if data_unique == {2}:\n",
    "      percentage_error = np.empty(energy.shape)*np.nan\n",
    "    elif data_unique == {3}:\n",
    "      percentage_error = np.array([item[1] for item in data])\n",
    "    return energy, counts, percentage_error\n",
    "  else:\n",
    "    print('ERROR: At some point or all, spectrum data format does not match with either \"Energy\\tcounts\" or \"Energy\\tcounts\\tError\".')\n",
    "    exit()\n",
    "  print('ERROR: cannot load spectrum data')\n",
    "  exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization:\n",
    "class_labels = {\n",
    "  'bkg':   0,\n",
    "  'Ba133': 1,\n",
    "  'Cs137': 2,\n",
    "  'Co60':  3\n",
    "}\n",
    "num_classes = len(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['D0', 'train']\n",
      "#datapoints in class-0: 999\n",
      "#datapoints in class-1: 1000\n",
      "#datapoints in class-2: 1006\n",
      "#datapoints in class-3: 995\n",
      "\n",
      " ['D1', 'test']\n",
      "#datapoints in class-0: 1002\n",
      "#datapoints in class-1: 1004\n",
      "#datapoints in class-2: 999\n",
      "#datapoints in class-3: 995\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "for combination in [['D0', 'train'], ['D1', 'test']]:\n",
    "  print('\\n', combination)\n",
    "  directory, datafilename = combination[0], combination[1]\n",
    "\n",
    "  csv = []  # to collect produced spectrums\n",
    "  ID = -1\n",
    "\n",
    "  rn_tag = 'bkg_Exp_base'\n",
    "  filename = [item for item in os.listdir('base_spectrums') if rn_tag in item][0]\n",
    "  spect0 = load_spectrum(f'base_spectrums/{filename}')\n",
    "\n",
    "  # signal_spectrum:\n",
    "  for RN in [*class_labels]:\n",
    "    rn_tag = 'bkg_Exp_base' if RN == 'bkg' else f'{RN}_FLUKA_base'\n",
    "    filename = [item for item in os.listdir('base_spectrums') if rn_tag in item][0]\n",
    "    spect1 = load_spectrum(f'base_spectrums/{filename}')\n",
    "\n",
    "    '''\n",
    "    NOTE:\n",
    "    min and max energy values are same for simulated and experimental data\n",
    "    #energy_bins (i.e #channels or #features) are also same for simulated and experimental data\n",
    "    however, the energy bin boundaries are not exactly same\n",
    "    hence, for mixing the spectrums (simulated + experimental), this should be considered.\n",
    "    (class-0 is always experimenal data since background can be simulated as its origin is natural radioactivity in environment)\n",
    "    '''\n",
    "    ene_pairs = []\n",
    "    for i in range(len(spect0[0])):\n",
    "      j = np.argmin(np.abs(spect1[0] - spect0[0][i]))\n",
    "      ene_pairs.append([i,j])\n",
    "    \n",
    "    # normalize spect w.r.t total counts:\n",
    "    spect0_norm = [spect0[0], spect0[1]/spect0[1].sum(), spect0[2]]\n",
    "    spect1_norm = [spect1[0], spect1[1]/spect1[1].sum(), spect1[2]]\n",
    "    \n",
    "    if directory == 'D0':\n",
    "      np.random.seed(0)\n",
    "    elif directory == 'D1':\n",
    "      np.random.seed(10)\n",
    "    \n",
    "    '''\n",
    "    Now,\n",
    "    the SNR (signal to noise mixing ratio) ~ random.uniform(low=0.05, high=1.1)\n",
    "    '''\n",
    "    \n",
    "    SNR_list = [0.05] + list(np.random.uniform(low=0.05, high=1.1, size=30)) + [1.1]\n",
    "    # the superimposed spetrum:\n",
    "    # energy-wise addition:\n",
    "    for SNR in SNR_list:\n",
    "      counts = []\n",
    "      for ene in ene_pairs:\n",
    "        counts.append(spect0_norm[1][ene[0]] + SNR*spect1_norm[1][ene[1]])\n",
    "      counts = np.array(counts)\n",
    "      spect_norm = [spect0[0], counts/counts.sum(), spect0[2]]\n",
    "      \n",
    "      '''\n",
    "      Now,\n",
    "      the gross count variation:\n",
    "      '''\n",
    "      gross_counts_list = [500] + list(np.random.uniform(low=500, high=10000, size=30)) + [10000]\n",
    "      for gross_counts in gross_counts_list:\n",
    "        # note: at this point, counts have been scaled but the statistics has not changed.\n",
    "        # to change the data_statistics, we resample each count from Poission distribution with new_count as expected_count\n",
    "        good_spect = [spect_norm[0], spect_norm[1]*gross_counts, spect_norm[2]]\n",
    "        counts = []\n",
    "        for i in range(len(good_spect[1])):\n",
    "          counts.append(np.random.poisson(lam=good_spect[1][i]))\n",
    "        counts = np.array(counts)\n",
    "        bad_spect_norm = [spect_norm[0], counts/counts.sum(), spect_norm[2]]\n",
    "    \n",
    "        ID += 1\n",
    "        csv.append([ID] + [f'{item}' for item in bad_spect_norm[1]] + [class_labels[RN]] + [SNR] + [gross_counts])\n",
    "\n",
    "\n",
    "  np.random.shuffle(csv)\n",
    "  np.random.shuffle(csv)\n",
    "  csv = csv[:1000*num_classes]\n",
    "\n",
    "  csv = [['ID'] + [f'feature_{item}' for item in range(1016)] + ['label'] + ['SNR'] + ['gross_counts']] + csv\n",
    "\n",
    "  f = open(f'{directory}/{datafilename}{directory}_detailed.csv', 'w')\n",
    "  f.write('\\n'.join([','.join([str(item2) for item2 in item]) for item in csv]))\n",
    "  f.close()\n",
    "\n",
    "  f = open(f'{directory}/{datafilename}{directory}.csv', 'w')\n",
    "  f.write('\\n'.join([','.join([str(item2) for item2 in item[:-2]]) for item in csv]))\n",
    "  f.close()\n",
    "\n",
    "  # print number of data generated for each class:\n",
    "  f = open(f'{directory}/{datafilename}{directory}.csv', 'r')\n",
    "  data = [int(item.split(',')[-1]) for item in f.read().split('\\n')[1:]]\n",
    "  f.close()\n",
    "\n",
    "  for i in range(num_classes):\n",
    "    print(f'#datapoints in class-{i}:', len([item for item in data if item == i]))\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating experimental data with (olny) SNR variation:\n",
    "\n",
    "directory, datafilename = 'D2', 'test'\n",
    "\n",
    "csv = []  # to collect produced spectrums\n",
    "ID = -1\n",
    "\n",
    "rn_tag = 'bkg_Exp_base'\n",
    "filename = [item for item in os.listdir('base_spectrums') if rn_tag in item][0]\n",
    "spect0 = load_spectrum(f'base_spectrums/{filename}')\n",
    "\n",
    "# signal_spectrum:\n",
    "for RN in [*class_labels]:\n",
    "  rn_tag = f'{RN}_Exp_base'\n",
    "  filename = [item for item in os.listdir('base_spectrums') if rn_tag in item][0]\n",
    "  spect1 = load_spectrum(f'base_spectrums/{filename}')\n",
    "  # since, the during the experimental aquisition of data,\n",
    "  # 'bkg' (class-0) is inherently included in all signals (other classes)\n",
    "  # hence, for the purpose to generate a fixed SNR spectrum, the original 'bkg' should be subtracted:\n",
    "  if RN != 'bkg': spect1 = [spect1[0], spect1[1]-spect0[1], spect1[2]]  # bkg subtraction for Exp spectrums\n",
    "\n",
    "  # normalize spect w.r.t total counts:\n",
    "  spect0_norm = [spect0[0], spect0[1]/spect0[1].sum(), spect0[2]]\n",
    "  spect1_norm = [spect1[0], spect1[1]/spect1[1].sum(), spect1[2]]\n",
    "\n",
    "  if directory == 'D2':\n",
    "    np.random.seed(20)\n",
    "\n",
    "  '''\n",
    "  Now,\n",
    "  the SNR (signal to noise mixing ratio) ~ random.uniform(low=0.05, high=1.1)\n",
    "  '''\n",
    "  SNR_list = list(np.random.uniform(low=0.05, high=1.1, size=1000))\n",
    "\n",
    "  # the superimposed spetrum:\n",
    "  # energy-wise addition:\n",
    "  for SNR in SNR_list:\n",
    "    counts = spect0_norm[1] + SNR*spect1_norm[1]\n",
    "    spect_norm = [spect0[0], counts/counts.sum(), spect0[2]]\n",
    "    \n",
    "    ID += 1\n",
    "    csv.append([ID] + [f'{item}' for item in spect_norm[1]] + [class_labels[RN]] + [SNR] + [''])\n",
    "\n",
    "\n",
    "np.random.shuffle(csv)\n",
    "np.random.shuffle(csv)\n",
    "\n",
    "csv = [['ID'] + [f'feature_{item}' for item in range(1016)] + ['label'] + ['SNR'] + ['gross_counts']] + csv\n",
    "\n",
    "f = open(f'{directory}/{datafilename}{directory}_detailed.csv', 'w')\n",
    "f.write('\\n'.join([','.join([str(item2) for item2 in item]) for item in csv]))\n",
    "f.close()\n",
    "\n",
    "f = open(f'{directory}/{datafilename}{directory}.csv', 'w')\n",
    "f.write('\\n'.join([','.join([str(item2) for item2 in item[:-2]]) for item in csv]))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generation: experimental + counting_statistics variation\n",
    "\n",
    "directory, datafilename = 'D3', 'test'\n",
    "\n",
    "csv = []  # to collect produced spectrums\n",
    "ID = -1\n",
    "\n",
    "for RN in [*class_labels]:\n",
    "  rn_tag = f'{RN}_Exp_base'\n",
    "  filename = [item for item in os.listdir('base_spectrums') if rn_tag in item][0]\n",
    "  spect1 = load_spectrum(f'base_spectrums/{filename}')\n",
    "  \n",
    "  # normalize spect w.r.t total counts:\n",
    "  spect1_norm = [spect1[0], spect1[1]/spect1[1].sum(), spect1[2]]\n",
    "\n",
    "  if directory == 'D3':\n",
    "    np.random.seed(30)\n",
    "\n",
    "  '''\n",
    "  Now,\n",
    "  the gross count variation:\n",
    "  '''\n",
    "  gross_counts_list = list(np.random.uniform(low=500, high=10000, size=1000))\n",
    "  for gross_counts in gross_counts_list:\n",
    "    good_spect = [spect1_norm[0], spect1_norm[1]*gross_counts, spect1_norm[2]]\n",
    "    counts = []\n",
    "    for i in range(len(good_spect[1])):\n",
    "      counts.append(np.random.poisson(lam=good_spect[1][i]))\n",
    "    counts = np.array(counts)\n",
    "    bad_spect_norm = [spect1_norm[0], counts/counts.sum(), spect1_norm[2]]\n",
    "    \n",
    "    ID += 1\n",
    "    csv.append([ID] + [f'{item}' for item in bad_spect_norm[1]] + [class_labels[RN]] + [''] + [gross_counts])\n",
    "\n",
    "\n",
    "np.random.shuffle(csv)\n",
    "np.random.shuffle(csv)\n",
    "\n",
    "csv = [['ID'] + [f'feature_{item}' for item in range(1016)] + ['label'] + ['SNR'] + ['gross_counts']] + csv\n",
    "\n",
    "f = open(f'{directory}/{datafilename}{directory}_detailed.csv', 'w')\n",
    "f.write('\\n'.join([','.join([str(item2) for item2 in item]) for item in csv]))\n",
    "f.close()\n",
    "\n",
    "f = open(f'{directory}/{datafilename}{directory}.csv', 'w')\n",
    "f.write('\\n'.join([','.join([str(item2) for item2 in item[:-2]]) for item in csv]))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "drfit = -5\n",
      "class-0: 1001\n",
      "class-1: 1000\n",
      "class-2: 1001\n",
      "class-3: 998\n",
      "\n",
      "drfit = -50\n",
      "class-0: 1001\n",
      "class-1: 1000\n",
      "class-2: 1001\n",
      "class-3: 998\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "generating dataset: D4\n",
    "'''\n",
    "\n",
    "directory, datafilename = 'D42', 'test'\n",
    "\n",
    "drift_list = [-5, -50]\n",
    "\n",
    "for drift in drift_list:\n",
    "  print('\\ndrfit =', drift)\n",
    "\n",
    "  csv = []  # to collect produced spectrums\n",
    "  ID = -1\n",
    "\n",
    "  if directory == 'D42':\n",
    "    np.random.seed(42)\n",
    "\n",
    "  # the SNR (signal to noise mixing ratio) ~ random.uniform(low=0.05, high=1.1)\n",
    "  SNR_list = [0.05] + list(np.random.uniform(low=0.05, high=1.1, size=30)) + [1.1]\n",
    "\n",
    "  for SNR in SNR_list:\n",
    "\n",
    "    # the gross count variation:\n",
    "    gross_counts_list = [500] + list(np.random.uniform(low=500, high=10000, size=30)) + [10000]\n",
    "\n",
    "    for gross_counts in gross_counts_list:\n",
    "\n",
    "      # for each class:\n",
    "      for RN in [*class_labels]:\n",
    "\n",
    "        # noise:\n",
    "        rn_tag = 'bkg_Exp_base'\n",
    "        filename = [item for item in os.listdir('base_spectrums') if rn_tag in item][0]\n",
    "        spect0 = load_spectrum(f'base_spectrums/{filename}')\n",
    "        spect0_norm = [spect0[0], spect0[1]/spect0[1].sum(), spect0[2]]\n",
    "\n",
    "        # signal:\n",
    "        rn_tag = f'{RN}_Exp_base'\n",
    "        filename = [item for item in os.listdir('base_spectrums') if rn_tag in item][0]\n",
    "        spect1 = load_spectrum(f'base_spectrums/{filename}')\n",
    "        spect1_norm = [spect1[0], spect1[1]/spect1[1].sum(), spect1[2]]\n",
    "\n",
    "        # SNR:\n",
    "        counts = []\n",
    "        counts = spect0_norm[1] + SNR*spect1_norm[1]\n",
    "        spect_norm = [spect0[0], counts/counts.sum(), spect0[2]]\n",
    "\n",
    "        # statistics:\n",
    "        good_spect = [spect_norm[0], spect_norm[1]*gross_counts, spect_norm[2]]\n",
    "        counts = []\n",
    "        for i in range(len(good_spect[1])):\n",
    "          counts.append(np.random.poisson(lam=good_spect[1][i]))\n",
    "        counts = np.array(counts)\n",
    "        bad_spect_norm = [spect_norm[0], counts/counts.sum(), spect_norm[2]]\n",
    "\n",
    "        # energy-drift:\n",
    "        old_energy = bad_spect_norm[0]\n",
    "        new_energy = old_energy*(1+drift/100)\n",
    "        interpolator = interp1d(bad_spect_norm[0], bad_spect_norm[1], kind='linear', fill_value=\"extrapolate\")\n",
    "        counts = interpolator(new_energy)\n",
    "        spect_norm = [bad_spect_norm[0], counts/counts.sum(), bad_spect_norm[2]]\n",
    "\n",
    "        ID += 1\n",
    "        csv.append([ID] + [f'{item}' for item in spect_norm[1]] + [class_labels[RN]] + [SNR] + [gross_counts] + [drift])\n",
    "\n",
    "  np.random.shuffle(csv)\n",
    "  np.random.shuffle(csv)\n",
    "  csv = csv[:1000*num_classes]\n",
    "\n",
    "  csv = [['ID'] + [f'feature_{item}' for item in range(1016)] + ['label'] + ['SNR'] + ['gross_counts'] + ['drift%']] + csv\n",
    "\n",
    "  f = open(f'{directory}/{datafilename}{directory}_{drift}percent_detailed.csv', 'w')\n",
    "  f.write('\\n'.join([','.join([str(item2) for item2 in item]) for item in csv]))\n",
    "  f.close()\n",
    "\n",
    "  f = open(f'{directory}/{datafilename}{directory}_{drift}percent.csv', 'w')\n",
    "  f.write('\\n'.join([','.join([str(item2) for item2 in item[:-3]]) for item in csv]))\n",
    "  f.close()\n",
    "\n",
    "  f = open(f'{directory}/{datafilename}{directory}_{drift}percent.csv', 'r')\n",
    "  data = [int(item.split(',')[-1]) for item in f.read().split('\\n')[1:]]\n",
    "  f.close()\n",
    "\n",
    "  for i in range(num_classes):\n",
    "    print(f'class-{i}:', len([item for item in data if item == i]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iitb_assgmt1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
